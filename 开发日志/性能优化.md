首先,压测结果不是很让人满意
```bash
hey -n 5000 -c 100 http://localhost:8080/resolve/EB1EE2YmD7Q

Summary:
Total:        9.0524 secs
Slowest:      1.6760 secs
Fastest:      0.0174 secs
Average:      0.1678 secs
Requests/sec: 552.3384


Response time histogram:
0.017 [1]     |
0.183 [3887]  |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
0.349 [738]   |■■■■■■■■
0.515 [223]   |■■
0.681 [85]    |■
0.847 [46]    |
1.013 [10]    |
1.178 [1]     |
1.344 [7]     |
1.510 [0]     |
1.676 [2]     |


Latency distribution:
10% in 0.0809 secs
25% in 0.1072 secs
50% in 0.1338 secs
75% in 0.1739 secs
90% in 0.2979 secs
95% in 0.3951 secs
99% in 0.7610 secs

Details (average, fastest, slowest):
DNS+dialup:   0.0008 secs, 0.0174 secs, 1.6760 secs
DNS-lookup:   0.0004 secs, 0.0000 secs, 0.0519 secs
req write:    0.0001 secs, 0.0000 secs, 0.0198 secs
resp wait:    0.0453 secs, 0.0069 secs, 1.1593 secs
resp read:    0.0170 secs, 0.0002 secs, 1.3436 secs

Status code distribution:
[200] 5000 responses
```

然后根据jaeger,发现发送事件占用了大量的事件,再通过prometheus,查到了为什么发送事件占用了大量的时间:
```text
# TYPE link_service_mq_publish_duration_seconds histogram
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.005"} 4899
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.01"} 4969
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.025"} 4991
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.05"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.1"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.25"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="0.5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="1"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="2.5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="10"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click",le="+Inf"} 5000
link_service_mq_publish_duration_seconds_sum{event_type="click"} 3.218346699999994
link_service_mq_publish_duration_seconds_count{event_type="click"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.005"} 167
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.01"} 637
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.025"} 2392
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.05"} 4634
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.1"} 4986
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.25"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="0.5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="1"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="2.5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="5"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="10"} 5000
link_service_mq_publish_duration_seconds_bucket{event_type="click-all",le="+Inf"} 5000
link_service_mq_publish_duration_seconds_sum{event_type="click-all"} 140.04157369999993
link_service_mq_publish_duration_seconds_count{event_type="click-all"} 5000
# HELP link_service_mq_publish_total 消息队列事件发送请求情况
# TYPE link_service_mq_publish_total counter
link_service_mq_publish_total{event_type="click",result="success"} 5000
```

Click-All 耗时远大于 Click 耗时
link_service_mq_publish_duration_seconds{event_type="click"}: 仅测量 channel.PublishWithContext 的时间。
    平均耗时 (Publish): 3.218s/5000≈0.64ms。这个耗时非常低，表明实际的消息发送操作非常快。
link_service_mq_publish_duration_seconds{event_type="click-all"}: 测量从 创建 Channel (start0) 到 发送完成 (end) 的总时间。
    总耗时 (All): 140.04s/5000≈28.0ms。
Channel 创建及清理时间=总耗时−发送耗时≈28.0ms−0.64ms≈27.36ms

创建和关闭 RabbitMQ Channel 的开销占据了服务约 45% 的核心处理时间

然后用pprof来看一下:
```text
(pprof) top
Showing nodes accounting for 12.41s, 56.67% of 21.90s total
Dropped 696 nodes (cum <= 0.11s)
Showing top 10 nodes out of 238
      flat  flat%   sum%        cum   cum%
     7.57s 34.57% 34.57%      7.74s 35.34%  runtime.cgocall
     1.97s  9.00% 43.56%      1.97s  9.00%  runtime.stdcall0
     1.16s  5.30% 48.86%      1.16s  5.30%  runtime.stdcall2
     0.38s  1.74% 50.59%      0.38s  1.74%  runtime.stdcall1
     0.30s  1.37% 51.96%      0.30s  1.37%  runtime.stdcall6
     0.27s  1.23% 53.20%      0.27s  1.23%  runtime.memclrNoHeapPointers
     0.21s  0.96% 54.16%      0.21s  0.96%  runtime.nextFreeFast (inline)
     0.20s  0.91% 55.07%      0.56s  2.56%  runtime.selectgo
     0.18s  0.82% 55.89%      0.96s  4.38%  runtime.newobject
     0.17s  0.78% 56.67%      0.17s  0.78%  runtime.memmove
```
有 超过 1/3 的 CPU 时间在跑 CGo，不是 Go 代码本身在跑
而 gRPC 处理链本身几乎没有成本


有几个方案
1. 将 MQ 发布改为异步处理
2. RabbitMQ Channel 复用
3. 直接改用kafka

我先尝试了异步处理(其实就是另建一个goroutine来处理事件发送逻辑)
在之前的同步模式下，创建/关闭 Channel 耗时约 28ms。现在在 异步 Goroutine 中执行，这个操作的平均耗时飙升到了 158.2ms
可能大量的goroutine造成了相当的调度压力

所以应该要做的是搞一个生产者消费者模式的内存队列,然后把消息放到里面,消费者从里面拿出来之后再进行事件的发送.这也同时完成了Channel的复用

但是这和kafka做的也差不多,所以还是改用kafka罢,本来这个应用场景就更适合kafka

---

我换成了kafka,修改了采样策略(10%),测试时暂时不跳转(直接返回200)
```text
hey -z 40s -c 100 http://localhost:8080/resolve/JqqPnSUsApr

Summary:
  Total:        40.0195 secs
  Slowest:      1.4262 secs
  Fastest:      0.0013 secs
  Average:      0.0693 secs
  Requests/sec: 1441.8714

  Total data:   115406 bytes
  Size/request: 2 bytes

Response time histogram:
  0.001 [1]     |
  0.144 [53751] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.286 [3370]  |■■■
  0.429 [387]   |
  0.571 [90]    |
  0.714 [14]    |
  0.856 [70]    |
  0.999 [1]     |
  1.141 [9]     |
  1.284 [4]     |
  1.426 [6]     |


Latency distribution:
  10% in 0.0027 secs
  25% in 0.0443 secs
  50% in 0.0598 secs
  75% in 0.0845 secs
  90% in 0.1281 secs
  95% in 0.1583 secs
  99% in 0.2878 secs

Details (average, fastest, slowest):
  DNS+dialup:   0.0001 secs, 0.0013 secs, 1.4262 secs
  DNS-lookup:   0.0001 secs, 0.0000 secs, 0.0592 secs
  req write:    0.0000 secs, 0.0000 secs, 0.0131 secs
  resp wait:    0.0691 secs, 0.0012 secs, 1.3681 secs
  resp read:    0.0001 secs, 0.0000 secs, 0.0340 secs

Status code distribution:
  [200] 57703 responses
```

性能的确有不少提升,再来看看pprof
```text
(pprof) top 10
Showing nodes accounting for 13.55s, 47.61% of 28.46s total
Dropped 797 nodes (cum <= 0.14s)
Showing top 10 nodes out of 259
      flat  flat%   sum%        cum   cum%
     7.32s 25.72% 25.72%      7.45s 26.18%  runtime.cgocall
     2.13s  7.48% 33.20%      2.13s  7.48%  runtime.stdcall0
     1.30s  4.57% 37.77%      1.30s  4.57%  runtime.stdcall2
     0.59s  2.07% 39.85%      0.60s  2.11%  runtime.stdcall1
     0.48s  1.69% 41.53%      0.48s  1.69%  runtime.nextFreeFast (inline)
     0.45s  1.58% 43.11%      0.46s  1.62%  runtime.stdcall6
     0.37s  1.30% 44.41%      1.30s  4.57%  runtime.newobject
     0.36s  1.26% 45.68%      0.36s  1.26%  runtime.(*mspan).writeHeapBitsSmall
     0.32s  1.12% 46.80%      0.32s  1.12%  runtime.memclrNoHeapPointers
     0.23s  0.81% 47.61%      0.23s  0.81%  strings.ToLower
```

还是runtime.cgocall
火焰图上标出了链条:transport NewServerTransport func3 -> transport (*loopyWriter) run -> net(*conn) Write -> poll execIO -> syscall Syscall9 -> runtime syscall_syscalln --> runtime cgocall 

ai说:
业务上下文:
transport NewServerTransport func3 / transport (*loopyWriter) run： 
这很可能与您的 gRPC 服务端 或 Kafka 客户端（如果是 Producer） 内部的网络传输机制有关
loopyWriter 模式常见于 gRPC 或 HTTP/2 协议栈，负责后台将数据写入连接

核心瓶颈点:
net(*conn) Write → poll execIO： 这是 Golang 进行网络 I/O 的标准流程，通过网络连接写入数据
syscall Syscall9 / runtime syscall_syscalln： 这是 Golang 切换到操作系统内核执行系统调用的标准流程
致命环节： runtime syscall_syscalln → runtime cgocall
正常情况下： Go 的系统调用 (syscall) 会直接结束，不需要调用 cgocall
现在的调用链（Syscall → Cgocall）： 这意味着您的程序在进行网络写入（net.Write）时，Go 的标准网络库或其依赖的底层 I/O 机制 不得不切换到 CGO 环境去执行，这可能是因为：
- 您正在 Windows 上运行或测试。 在 Windows 环境下，Go 对某些 I/O 操作的封装可能仍然依赖于 CGO shim。
- 您使用的 gRPC/网络库/Kafka 库确实依赖 CGO

---
这原来是windows的锅吗?

在 Linux 上，Go runtime 可以直接高效地使用 epoll 或类似的异步 I/O 机制来执行网络操作，这是纯 Go 的。

在 Windows 上，Go runtime 为了兼容 Windows 的底层网络 API (如 WinSock 或 IO Completion Ports)，在某些并发密集型的 I/O 操作中，会涉及更多的 CGO 封装和系统调用。

我使用windows进行开发,为了开发测试的方便,使用docker-compose部署基础设施,在本机上直接使用`go run main.go`来运行服务

---

```text
hey -n 10000 -c 100 http://localhost:8080/resolve/3AGsV2ChxUG

Summary:
  Total:        3.6701 secs
  Slowest:      0.3852 secs
  Fastest:      0.0015 secs
  Average:      0.0352 secs
  Requests/sec: 2724.7421

  Total data:   20000 bytes
  Size/request: 2 bytes

Response time histogram:
  0.001 [1]     |
  0.040 [7309]  |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.078 [2548]  |■■■■■■■■■■■■■■
  0.117 [42]    |
  0.155 [2]     |
  0.193 [0]     |
  0.232 [0]     |
  0.270 [28]    |
  0.308 [14]    |
  0.347 [21]    |
  0.385 [35]    |


Latency distribution:
  10% in 0.0164 secs
  25% in 0.0229 secs
  50% in 0.0313 secs
  75% in 0.0407 secs
  90% in 0.0516 secs
  95% in 0.0589 secs
  99% in 0.1393 secs

Details (average, fastest, slowest):
  DNS+dialup:   0.0002 secs, 0.0015 secs, 0.3852 secs
  DNS-lookup:   0.0001 secs, 0.0000 secs, 0.0202 secs
  req write:    0.0001 secs, 0.0000 secs, 0.0184 secs
  resp wait:    0.0347 secs, 0.0014 secs, 0.3697 secs
  resp read:    0.0002 secs, 0.0000 secs, 0.0063 secs

Status code distribution:
  [200] 10000 responses
```